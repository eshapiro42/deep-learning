
<head>
    <meta charset="utf-8">

    <title>Linear Regression with Neurons (2)</title>

    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">

    <link rel="shortcut icon" href="../../favicon.ico" type="image/x-icon">
    <link rel="canonical" href="../index.html">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <meta property="og:site_name" content="Deep Learning">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Linear Regression with Neurons (2)">
    <meta property="og:description" content="More complicated data
Last time, I introduced a simple artificial neuron with a single parameter, $w$.



We figured out how, given labeled training data[1]  that was reasonably linear
and passed through the origin, this neuron could learn to make reasonable
predictions. The key to this was defining a cost function to measure how poorly
the neuron was performing for a given weight $w$, and then iteratively applying
gradient descent to obtain new values of $w$, lower the cost and improve
performa">
    <meta property="og:url" content="http://localhost:2368/linear-regression-with-neurons-2/">
    <meta property="og:image" content="https://images.unsplash.com/photo-1499626662328-b83a935441a4?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1080&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ">
    <meta property="article:published_time" content="2019-05-29T23:09:00.000Z">
    <meta property="article:modified_time" content="2019-05-30T03:50:11.000Z">
    <meta property="article:publisher" content="https://www.facebook.com/ghost">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Linear Regression with Neurons (2)">
    <meta name="twitter:description" content="More complicated data
Last time, I introduced a simple artificial neuron with a single parameter, $w$.



We figured out how, given labeled training data[1]  that was reasonably linear
and passed through the origin, this neuron could learn to make reasonable
predictions. The key to this was defining a cost function to measure how poorly
the neuron was performing for a given weight $w$, and then iteratively applying
gradient descent to obtain new values of $w$, lower the cost and improve
performa">
    <meta name="twitter:url" content="http://localhost:2368/linear-regression-with-neurons-2/">
    <meta name="twitter:image" content="https://images.unsplash.com/photo-1499626662328-b83a935441a4?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1080&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="Eric Shapiro">
    <meta name="twitter:site" content="@tryghost">
    <meta property="og:image:width" content="1080">
    <meta property="og:image:height" content="1072">
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Deep Learning",
        "logo": {
            "@type": "ImageObject",
            "url": "http://localhost:2368/favicon.ico",
            "width": 60,
            "height": 60
        }
    },
    "author": {
        "@type": "Person",
        "name": "Eric Shapiro",
        "url": "http://localhost:2368/author/eric/",
        "sameAs": []
    },
    "headline": "Linear Regression with Neurons (2)",
    "url": "http://localhost:2368/linear-regression-with-neurons-2/",
    "datePublished": "2019-05-29T23:09:00.000Z",
    "dateModified": "2019-05-30T03:50:11.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://images.unsplash.com/photo-1499626662328-b83a935441a4?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=1080&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ",
        "width": 1080,
        "height": 1072
    },
    "description": "More complicated data\nLast time, I introduced a simple artificial neuron with a single parameter, $w$.\n\n\n\nWe figured out how, given labeled training data[1]  that was reasonably linear\nand passed through the origin, this neuron could learn to make reasonable\npredictions. The key to this was defining a cost function to measure how poorly\nthe neuron was performing for a given weight $w$, and then iteratively applying\ngradient descent to obtain new values of $w$, lower the cost and improve\nperforma",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://localhost:2368/"
    }
}
    </script>

    <meta name="generator" content="Ghost 2.23">
    <link rel="alternate" type="application/rss+xml" title="Deep Learning" href="../../rss/index.html">

    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic%7COpen+Sans:700,600,400">
    <style amp-custom>html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,aside,details,figcaption,figure,footer,header,main,menu,nav,section,summary{display:block}audio,canvas,progress,video{display:inline-block;vertical-align:baseline}audio:not([controls]){display:none;height:0}[hidden],template{display:none}a{background-color:transparent}a:active,a:hover{outline:0}abbr[title]{border-bottom:1px dotted}b,strong{font-weight:bold}dfn{font-style:italic}h1{margin:0.67em 0;font-size:2em}mark{background:#ff0;color:#000}small{font-size:80%}sub,sup{position:relative;vertical-align:baseline;font-size:75%;line-height:0}sup{top:-0.5em}sub{bottom:-0.25em}img{border:0}amp-img{border:0}svg:not(:root){overflow:hidden}figure{margin:1em 40px}hr{box-sizing:content-box;height:0}pre{overflow:auto}code,kbd,pre,samp{font-family:monospace, monospace;font-size:1em}button,input,optgroup,select,textarea{margin:0;color:inherit;font:inherit}button{overflow:visible}button,select{text-transform:none}button,html input[type="button"],input[type="reset"],input[type="submit"]{cursor:pointer;-webkit-appearance:button}button[disabled],html input[disabled]{cursor:default}button::-moz-focus-inner,input::-moz-focus-inner{padding:0;border:0}input{line-height:normal}input[type="checkbox"],input[type="radio"]{box-sizing:border-box;padding:0}input[type="number"]::-webkit-inner-spin-button,input[type="number"]::-webkit-outer-spin-button{height:auto}input[type="search"]{-webkit-appearance:textfield}input[type="search"]::-webkit-search-cancel-button,input[type="search"]::-webkit-search-decoration{-webkit-appearance:none}fieldset{margin:0 2px;padding:0.35em 0.625em 0.75em;border:1px solid #c0c0c0}legend{padding:0;border:0}textarea{overflow:auto}optgroup{font-weight:bold}table{border-spacing:0;border-collapse:collapse}td,th{padding:0}html{max-height:100%;height:100%;font-size:62.5%;-webkit-tap-highlight-color:rgba(0, 0, 0, 0)}body{max-height:100%;height:100%;color:#3a4145;background:#f4f8fb;letter-spacing:0.01rem;font-family:"Merriweather", serif;font-size:1.8rem;line-height:1.75em;text-rendering:geometricPrecision;-webkit-font-feature-settings:"kern" 1;-moz-font-feature-settings:"kern" 1;-o-font-feature-settings:"kern" 1}::-moz-selection{background:#d6edff}::selection{background:#d6edff}h1,h2,h3,h4,h5,h6{margin:0 0 0.3em 0;color:#2e2e2e;font-family:"Open Sans", sans-serif;line-height:1.15em;text-rendering:geometricPrecision;-webkit-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1;-moz-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1;-o-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1}h1{text-indent:-2px;letter-spacing:-1px;font-size:2.6rem}h2{letter-spacing:0;font-size:2.4rem}h3{letter-spacing:-0.6px;font-size:2.1rem}h4{font-size:1.9rem}h5{font-size:1.8rem}h6{font-size:1.8rem}a{color:#4a4a4a}a:hover{color:#111}p,ul,ol,dl{margin:0 0 2.5rem 0;font-size:1.5rem;text-rendering:geometricPrecision;-webkit-font-feature-settings:"liga" 1, "onum" 1, "kern" 1;-moz-font-feature-settings:"liga" 1, "onum" 1, "kern" 1;-o-font-feature-settings:"liga" 1, "onum" 1, "kern" 1}ol,ul{padding-left:2em}ol ol,ul ul,ul ol,ol ul{margin:0 0 0.4em 0;padding-left:2em}dl dt{float:left;clear:left;overflow:hidden;margin-bottom:1em;width:180px;text-align:right;text-overflow:ellipsis;white-space:nowrap;font-weight:700}dl dd{margin-bottom:1em;margin-left:200px}li{margin:0.4em 0}li li{margin:0}hr{display:block;margin:1.75em 0;padding:0;height:1px;border:0;border-top:#efefef 1px solid}blockquote{box-sizing:border-box;margin:1.75em 0 1.75em 0;padding:0 0 0 1.75em;border-left:#4a4a4a 0.4em solid;-moz-box-sizing:border-box}blockquote p{margin:0.8em 0;font-style:italic}blockquote small{display:inline-block;margin:0.8em 0 0.8em 1.5em;color:#ccc;font-size:0.9em}blockquote small:before{content:"\2014 \00A0"}blockquote cite{font-weight:700}blockquote cite a{font-weight:normal}mark{background-color:#fdffb6}code,tt{padding:1px 3px;border:#e3edf3 1px solid;background:#f7fafb;border-radius:2px;white-space:pre-wrap;font-family:Inconsolata, monospace, sans-serif;font-size:0.85em;font-feature-settings:"liga" 0;-webkit-font-feature-settings:"liga" 0;-moz-font-feature-settings:"liga" 0}pre{overflow:auto;box-sizing:border-box;margin:0 0 1.75em 0;padding:10px;width:100%;border:#e3edf3 1px solid;background:#f7fafb;border-radius:3px;white-space:pre;font-family:Inconsolata, monospace, sans-serif;font-size:0.9em;-moz-box-sizing:border-box}pre code,pre tt{padding:0;border:none;background:transparent;white-space:pre-wrap;font-size:inherit}kbd{display:inline-block;margin-bottom:0.4em;padding:1px 8px;border:#ccc 1px solid;background:#f4f4f4;border-radius:4px;box-shadow:0 1px 0 rgba(0, 0, 0, 0.2), 0 1px 0 0 #fff inset;color:#666;text-shadow:#fff 0 1px 0;font-size:0.9em;font-weight:700}table{box-sizing:border-box;margin:1.75em 0;max-width:100%;width:100%;background-color:transparent;-moz-box-sizing:border-box}table th,table td{padding:8px;border-top:#efefef 1px solid;vertical-align:top;text-align:left;line-height:20px}table th{color:#000}table caption + thead tr:first-child th,table caption + thead tr:first-child td,table colgroup + thead tr:first-child th,table colgroup + thead tr:first-child td,table thead:first-child tr:first-child th,table thead:first-child tr:first-child td{border-top:0}table tbody + tbody{border-top:#efefef 2px solid}table table table{background-color:#fff}table tbody > tr:nth-child(odd) > td,table tbody > tr:nth-child(odd) > th{background-color:#f6f6f6}table.plain tbody > tr:nth-child(odd) > td,table.plain tbody > tr:nth-child(odd) > th{background:transparent}iframe,amp-iframe,.fluid-width-video-wrapper{display:block;margin:1.75em 0}.fluid-width-video-wrapper iframe,.fluid-width-video-wrapper amp-iframe{margin:0}textarea,select,input{margin:0 0 5px 0;padding:6px 9px;width:260px;outline:0;border:#e7eef2 1px solid;background:#fff;border-radius:4px;box-shadow:none;font-family:"Open Sans", sans-serif;font-size:1.6rem;line-height:1.4em;font-weight:100;-webkit-appearance:none}textarea{min-width:250px;min-height:80px;max-width:340px;width:100%;height:auto}input[type="text"]:focus,input[type="email"]:focus,input[type="search"]:focus,input[type="tel"]:focus,input[type="url"]:focus,input[type="password"]:focus,input[type="number"]:focus,input[type="date"]:focus,input[type="month"]:focus,input[type="week"]:focus,input[type="time"]:focus,input[type="datetime"]:focus,input[type="datetime-local"]:focus,textarea:focus{outline:none;outline-width:0;border:#bbc7cc 1px solid;background:#fff}select{width:270px;height:30px;line-height:30px}.clearfix:before,.clearfix:after{content:" ";display:table}.clearfix:after{clear:both}.clearfix{zoom:1}.main-header{position:relative;display:table;overflow:hidden;box-sizing:border-box;width:100%;height:50px;background:#5ba4e5 no-repeat center center;background-size:cover;text-align:left;-webkit-box-sizing:border-box;-moz-box-sizing:border-box}.content{background:#fff;padding-top:15px}.blog-title,.content{margin:auto;max-width:600px}.blog-title a{display:block;padding-right:16px;padding-left:16px;height:50px;color:#fff;text-decoration:none;font-family:"Open Sans", sans-serif;font-size:16px;line-height:50px;font-weight:600}.post{position:relative;margin-top:0;margin-right:16px;margin-left:16px;padding-bottom:0;max-width:100%;border-bottom:#ebf2f6 1px solid;word-wrap:break-word;font-size:0.95em;line-height:1.65em}.post-header{margin-bottom:1rem}.post-title{margin-bottom:0}.post-title a{text-decoration:none}.post-meta{display:block;margin:3px 0 0 0;color:#9eabb3;font-family:"Open Sans", sans-serif;font-size:1.3rem;line-height:2.2rem}.post-meta a{color:#9eabb3;text-decoration:none}.post-meta a:hover{text-decoration:underline}.post-meta .author{margin:0;font-size:1.3rem;line-height:1.3em}.post-date{display:inline-block;text-transform:uppercase;white-space:nowrap;font-size:1.2rem;line-height:1.2em}.post-image{margin:0;padding-top:3rem;padding-bottom:30px;border-top:1px #E8E8E8 solid}.post-content amp-img,.post-content amp-anim{position:relative;left:50%;display:block;padding:0;min-width:0;max-width:112%;width:calc(100% + 32px);height:auto;transform:translateX(-50%);-webkit-transform:translateX(-50%);-ms-transform:translateX(-50%)}.footnotes{font-size:1.3rem;line-height:1.6em;font-style:italic}.footnotes li{margin:0.6rem 0}.footnotes p{margin:0}.footnotes p a:last-child{text-decoration:none}.site-footer{position:relative;margin:0 auto 20px auto;padding:1rem 15px;max-width:600px;color:rgba(0,0,0,0.5);font-family:"Open Sans", sans-serif;font-size:1.1rem;line-height:1.75em}.site-footer a{color:rgba(0,0,0,0.5);text-decoration:none;font-weight:bold}.site-footer a:hover{border-bottom:#bbc7cc 1px solid}.poweredby{display:block;float:right;width:45%;text-align:right}.copyright{display:block;float:left;width:45%}</style>

    <style amp-boilerplate>body{-webkit-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-moz-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-ms-animation:-amp-start 8s steps(1,end) 0s 1 normal both;animation:-amp-start 8s steps(1,end) 0s 1 normal both}@-webkit-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-moz-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-ms-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-o-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}</style><noscript><style amp-boilerplate>body{-webkit-animation:none;-moz-animation:none;-ms-animation:none;animation:none}</style></noscript>
    <script async src="https://cdn.ampproject.org/v0.js"></script>

    <script async custom-element="amp-iframe" src="https://cdn.ampproject.org/v0/amp-iframe-0.1.js"></script>

</head>

<body class="amp-template">
    <header class="main-header">
        <nav class="blog-title">
            <a href="../../">Deep Learning</a>
        </nav>
    </header>

    <main class="content" role="main">
        <article class="post">

            <header class="post-header">
                <h1 class="post-title">Linear Regression with Neurons (2)</h1>
                <section class="post-meta">
                    <p class="author">by <a href="../../author/eric/">Eric Shapiro</a></p>
                    <time class="post-date" datetime="2019-05-29">2019-05-29</time>
                </section>
            </header>
            <figure class="post-image">
                <amp-img src="https://images.unsplash.com/photo-1499626662328-b83a935441a4?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1080&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" width="600" height="400" layout="responsive"></amp-img>
            </figure>
            <section class="post-content">

                <h1 id="morecomplicateddata">More complicated data</h1>
<p>Last time, I introduced a simple artificial neuron with a single parameter, $w$.</p>
<p></p>
<p>We figured out how, given labeled training data<sup class="footnote-ref"><a href="index.html#fn1" id="fnref1">[1]</a></sup> that was reasonably linear and passed through the origin, this neuron could learn to make reasonable predictions. The key to this was defining a cost function to measure how poorly the neuron was performing for a given weight $w$, and then iteratively applying gradient descent to obtain new values of $w$, lower the cost and improve performance. That learning process can be summarized as follows:</p>
<blockquote>
<ol>
<li>Choose an initial value for $w$.</li>
<li>Feed our labeled data to the neuron.</li>
<li>Compare each of the neuron's predictions to the true value from the training data.</li>
<li>Compute the gradient of the cost function from these predictions.</li>
<li>Apply one iteration of gradient descent to move $w$ in the direction that will lower the cost.</li>
<li>Go back to step 2 and repeat until you're satisfied with the neuron's predictions.</li>
</ol>
</blockquote>
<p>Now let's up it a notch. What if we still had linear training data, but it did not pass through the origin? Here's the data set we'll be working with for the remainder of this post:</p>
<amp-iframe width="100%" height="400" frameborder="0" src="https://plot.ly/~eshapiro/95.embed?showlink=false" sandbox="allow-scripts allow-same-origin" layout="responsive"></amp-iframe>
<p>Spoiler alert: I generated this data using random fluctuations from the line $y=-2x+5$. These fluctuations are normally distributed with a mean of $0$ and a standard deviation of $2$. This data is therefore well suited for linear regression! 😄</p>
<p>Our training data consists of 1000 points. In table form, the data looks like this (in no particular order):</p>
<p>$\begin{array}{|c|c|c|}<br>
\hline<br>
i &amp;   x           &amp; y \\<br>
\hline<br>
1 &amp;  -0.829779953 &amp; 8.11066094 \\<br>
\hline<br>
2 &amp;   2.20324493  &amp; -0.0548983078 \\<br>
\hline<br>
3 &amp;   -4.99885625 &amp; 16.6263988 \\<br>
\hline<br>
4 &amp;   -1.97667427 &amp; 10.5142884 \\<br>
\hline<br>
5 &amp;   -3.53244109 &amp; 9.13677504 \\<br>
\hline<br>
\vdots &amp; \vdots   &amp; \vdots \\<br>
\hline<br>
\end{array}$</p>
<p>Since this data does not go through the origin, you might expect that if we train our one-parameter neuron with this data, we will not get very good predictions. Let's give it a try.</p>
<amp-iframe height="600px" width="100%" src="https://repl.it/@EricShapiro/Linear-Regression-with-Neurons-2-1?lite=true" frameborder="no" allowtransparency="true" allowfullscreen="true" sandbox="allow-forms allow-pointer-lock allow-popups allow-same-origin allow-scripts allow-modals" layout="responsive"></amp-iframe>
<br>
<p>Here's the best fit line predicted by our one-parameter neuron after training on this data:</p>
<amp-iframe width="100%" height="400" frameborder="0" src="https://plot.ly/~eshapiro/99.embed?showlink=false" sandbox="allow-scripts allow-same-origin" layout="responsive"></amp-iframe>
<p>That's... not great. It did manage to get the slope right, so our neuron is really doing everything it can to minimize its cost. It just doesn't have the ability to make a prediction that <em>doesn't</em> go through the origin! So let's create a new neuron that does have this ability.</p>
<p></p>
<p>We haven't done anything groundbreaking here. All we've done is add a new parameter $b$ to the neuron which is also used in computing the neuron's output. We call $b$ the neuron's <strong>bias</strong>. Given an input value $x_i$, the neuron will now output the prediction $\hat{y_i} = wx_i+b$, using both the weight and the bias.</p>
<p>Our loss function stays the same for each data point,</p>
<p>$L(x_i) = (y_i - \hat{y_i})^2,$</p>
<p>as does our cost function for the entire neuron,</p>
<p>$C = \frac{1}{m}\sum_{i=1}^m L(x_i).$</p>
<p>(Recall that $m$ is the number of points in our training data.)</p>
<p>However, our predictions $\hat{y}$ are now calculated using a new formula. Since we've doubled the number of parameters in our neuron, we need to rethink how we do gradient descent.</p>
<h1 id="gradientdescentwithtwoparameters">Gradient descent with two parameters</h1>
<p>With two parameters, our cost function becomes a little more complicated. It's now a function of two variables, $w$ and $b$. If we were to visualize the cost function for our data over a range of different values of weights and biases, it would look like this:</p>
<amp-iframe width="100%" height="500" frameborder="0" src="https://plot.ly/~eshapiro/103.embed?showlink=false" sandbox="allow-scripts allow-same-origin" layout="responsive"></amp-iframe>
<p>If you move the surface around a bit, you'll notice that the optimal weight is roughly $-2$ and the optimal bias is roughly $5$, just as we'd expect.</p>
<p>Luckily, differential calculus doesn't get too much more complicated as we add dimensions. The gradient is no longer just the derivative of our cost function. It is really a vector of its partial derivatives. That is,</p>
<p>$\nabla C = \begin{bmatrix}<br>
\dfrac{\partial C}{\partial w} \\[.5em]<br>
\dfrac{\partial C}{\partial b}<br>
\end{bmatrix}.$</p>
<p>If this is gibberish to you, don't worry. You don't need to follow along with the math to get a sense of what's going on.</p>
<p>We calculate the partial derivatives much the same way we calculated the ordinary derivative last time — using the chain rule.</p>
<p>$\begin{align}<br>
\p{C}{w} &amp;= \sum_{i=1}^m \p{C}{L_i} \p{L_i}{\hat{y}_i} \p{\hat{y}_i}{w}, \\<br>
\p{C}{b} &amp;= \sum_{i=1}^m \p{C}{L_i} \p{L_i}{\hat{y}_i} \p{\hat{y}_i}{b}.<br>
\end{align}$</p>
<p>The intermediate partial derivatives are as follows:</p>
<p>$\begin{align}<br>
\p{C}{L_i} &amp;= \tfrac{1}{m}, \\<br>
\p{L_i}{\hat{y}_i} &amp;= -2(y_i-\hat{y}_i), \\<br>
\p{\hat{y}_i}{w} &amp;= x_i, \\<br>
\p{\hat{y}_i}{b} &amp;= 1.<br>
\end{align}$</p>
<p>Combining these using the chain rule, we see that</p>
<p>$\begin{align}<br>
\p{C}{w} &amp;= -\frac{2}{m} \sum_{i=1}^m (y_i - \hat{y}_i) x_i, \\<br>
\p{C}{b} &amp;= -\frac{2}{m} \sum_{i=1}^m (y_i - \hat{y}_i).<br>
\end{align}$</p>
<p>The partial derivative with respect to $w$ is actually the same as last time, and the derivative with respect to $b$ is actually simpler. Each component of the gradient tells us the direction of steepest ascent of the cost function $C$ in that direction. So, for instance, $\p{C}{w}$ gives the direction of steepest ascent in the $w$ direction at any point.</p>
<p>Our gradient descent algorithm doesn't have to change too much! We just update each variable independently according to its component of the gradient. That is, if $\alpha$ represents our chosen learning rate, we update our parameters according to the rules:</p>
<p>$\begin{align}<br>
w &amp;\to w - \alpha\p{C}{w} \\<br>
b &amp;\to b - \alpha\p{C}{b}.<br>
\end{align}$</p>
<p>If we iterate our gradient descent algorithm enough times with a good choice of learning rate and a little luck, we should always arrive at a choice of parameters $w$ and $b$ which minimizes the cost of our neuron!</p>
<p>As an aside, we can actually find the optimum cost analytically, just as we did last time. The mimimum value occurs where the gradient is zero, i.e., where each partial derivative is zero. This gives us two equations in two unknowns, which can be solved with some algebra. I will not go through the specifics, but the cost is minimized when</p>
<p>$\begin{align}<br>
w &amp;= \dfrac{m \sum_{i=1}^m x_i y_i - \sum_{i=1}^m x_i \sum_{i=1}^m y_i}{m \sum_{i=1}^m x_i^2 - \big(\sum_{i=1}^m x_i\big)^2}, \\[1em]<br>
b &amp;= \dfrac{\sum_{i=1}^m y_i \sum_{i=1}^m x_i^2 - \sum_{i=1}^m x_i \sum_{i=1}^m x_i y_i}{m \sum_{i=1}^m x_i^2 - \big(\sum_{i=1}^m x_i\big)^2}.<br>
\end{align}$</p>
<p>This technique of finding the optimal values of $w$ and $b$ analytically is called <strong>linear regression</strong>, and in the case of our training data it results in the following values:</p>
<p>$\begin{align}<br>
w &amp;= -1.99787325, \\<br>
b &amp;= 5.09462935.<br>
\end{align}$</p>
<h1 id="testingourneuron">Testing our neuron</h1>
<p>Let's see how close our two-parameter neuron comes to finding these optimal values!</p>
<p>The following is Python code which implements the algorithm for gradient descent that we've just described, updating the parameters $w$ and $b$ simultaneously according to their respective components of the gradient. Note that the <code>TwoParameterNeuron</code> class inherits from the same <code>Neuron</code> class that I defined in my last post.</p>

<br>
<p>If we train this neuron for 100 epochs on the training data defined at the beginning of this post, with a learning rate of 0.01, here's what happens:</p>
<amp-iframe height="600px" width="100%" src="https://repl.it/@EricShapiro/Linear-Regression-with-Neurons-2-2?lite=true" frameborder="no" allowtransparency="true" allowfullscreen="true" sandbox="allow-forms allow-pointer-lock allow-popups allow-same-origin allow-scripts allow-modals" layout="responsive"></amp-iframe>
<br>
<p>Uh oh! Our neuron reached a pretty good value for $w$, but that value for $b$ is still pretty far off. What happened?</p>
<p>Notice how the cost is still decreasing substantially with every iteration. Here's a plot of the neuron's cost as a function of the number of epochs it's trained for:</p>
<amp-iframe width="100%" height="400" frameborder="0" src="https://plot.ly/~eshapiro/105.embed?showlink=false" sandbox="allow-scripts allow-same-origin" layout="responsive"></amp-iframe>
<p>Okay, so maybe "substantially" was an overstatement, but the cost is definitely still going down! This implies that we haven't trained our neuron long enough. Let's try training it longer — we'll try 1000 epochs — and see if it does any better.</p>
<amp-iframe height="600px" width="100%" src="https://repl.it/@EricShapiro/Linear-Regression-with-Neurons-2-3?lite=true" frameborder="no" allowtransparency="true" allowfullscreen="true" sandbox="allow-forms allow-pointer-lock allow-popups allow-same-origin allow-scripts allow-modals" layout="responsive"></amp-iframe>
<br>
<p>Much better! Now our neuron's definitely on par with linear regression!</p>
<p>Here's another plot of the cost over these 1000 epochs of gradient descent:</p>
<amp-iframe width="100%" height="400" frameborder="0" src="https://plot.ly/~eshapiro/109.embed?showlink=false" sandbox="allow-scripts allow-same-origin" layout="responsive"></amp-iframe>
<p>The cost is definitely not decreasing by much anymore by the time 1000 epochs have passed. Notice how toward the beginning of training, the cost was decreasing a lot faster than toward the end. This is fairly common. In the early iterations of gradient descent, our parameters are typically very far from their targets and need to be adjusted by large amounts to compensate. Then as they begin to get better and better, gradient descent continues to fine-tune them but does not have quite as much to do anymore.</p>
<p>To wrap things up, let's take a look at how our two-parameter neuron is fitting our data after being trained for 1000 epochs:</p>
<amp-iframe width="100%" height="400" frameborder="0" src="https://plot.ly/~eshapiro/111.embed?showlink=false" sandbox="allow-scripts allow-same-origin" layout="responsive"></amp-iframe>
<p>I'd say that looks pretty good.</p>
<h1 id="whatsnext">What's next?</h1>
<p>That's about it for linear regression. In my next post, I'll talk about adding non-linear activations to our neurons and combining them to form larger networks!</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>By labeled training data, I mean data consisting of values for both the inputs and the corresponding observed outputs. This is the data that we want our neuron to learn from so it can generate reasonable predictions. <a href="index.html#fnref1" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>
<p></p><p></p>

            </section>

        </article>
    </main>
    <footer class="site-footer clearfix">
        <section class="copyright"><a href="../../">Deep Learning</a> © 2019</section>
        <section class="poweredby">Proudly published with <a href="https://ghost.org">Ghost</a></section>
    </footer>
</body>
