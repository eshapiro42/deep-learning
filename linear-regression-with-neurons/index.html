
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Linear Regression With Neurons (1)</title>
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="stylesheet" type="text/css" href="../assets/built/screen.css?v=8d5645c3cd">

    <link rel="shortcut icon" href="../favicon.ico" type="image/x-icon">
    <link rel="canonical" href="index.html">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <link rel="amphtml" href="amp/index.html">
    
    <meta property="og:site_name" content="Deep Learning">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Linear Regression With Neurons (1)">
    <meta property="og:description" content="What are neurons?

The goal of this post is to introduce artificial neurons  — the basic building
blocks of a neural network — as well as the way in which they learn. Their
general concept is based on their namesake, the neurons of the human brain.

Biological NeuronsI'm no expert in cognitive science, but from what I understand
a biological neuron receives a number of electrical impulses from surrounding
neurons. Then, based on the cumulative intensity of these signals, the neuron
might send ou">
    <meta property="og:url" content="http://localhost:2368/linear-regression-with-neurons/">
    <meta property="og:image" content="https://images.unsplash.com/photo-1511268594014-0e9d3ea5c33e?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1080&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ">
    <meta property="article:published_time" content="2019-05-28T02:42:02.000Z">
    <meta property="article:modified_time" content="2019-05-30T02:58:58.000Z">
    <meta property="article:publisher" content="https://www.facebook.com/ghost">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Linear Regression With Neurons (1)">
    <meta name="twitter:description" content="What are neurons?

The goal of this post is to introduce artificial neurons  — the basic building
blocks of a neural network — as well as the way in which they learn. Their
general concept is based on their namesake, the neurons of the human brain.

Biological NeuronsI'm no expert in cognitive science, but from what I understand
a biological neuron receives a number of electrical impulses from surrounding
neurons. Then, based on the cumulative intensity of these signals, the neuron
might send ou">
    <meta name="twitter:url" content="http://localhost:2368/linear-regression-with-neurons/">
    <meta name="twitter:image" content="https://images.unsplash.com/photo-1511268594014-0e9d3ea5c33e?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1080&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="Eric Shapiro">
    <meta name="twitter:site" content="@tryghost">
    <meta property="og:image:width" content="1080">
    <meta property="og:image:height" content="720">
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Deep Learning",
        "logo": {
            "@type": "ImageObject",
            "url": "http://localhost:2368/favicon.ico",
            "width": 60,
            "height": 60
        }
    },
    "author": {
        "@type": "Person",
        "name": "Eric Shapiro",
        "url": "http://localhost:2368/author/eric/",
        "sameAs": []
    },
    "headline": "Linear Regression With Neurons (1)",
    "url": "http://localhost:2368/linear-regression-with-neurons/",
    "datePublished": "2019-05-28T02:42:02.000Z",
    "dateModified": "2019-05-30T02:58:58.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://images.unsplash.com/photo-1511268594014-0e9d3ea5c33e?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=1080&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ",
        "width": 1080,
        "height": 720
    },
    "description": "What are neurons?\n\nThe goal of this post is to introduce artificial neurons  — the basic building\nblocks of a neural network — as well as the way in which they learn. Their\ngeneral concept is based on their namesake, the neurons of the human brain.\n\nBiological NeuronsI&#x27;m no expert in cognitive science, but from what I understand\na biological neuron receives a number of electrical impulses from surrounding\nneurons. Then, based on the cumulative intensity of these signals, the neuron\nmight send ou",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://localhost:2368/"
    }
}
    </script>

    <meta name="generator" content="Ghost 2.23">
    <link rel="alternate" type="application/rss+xml" title="Deep Learning" href="../rss/index.html">
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [["$", "$"], ["\\(", "\\)"]],
        processEscapes: true
    },
    TeX: {
    	Macros: {
      		R: "{\\mathbb{R}}",
            abs: ["{\\lvert #1 \\rvert}", 1],
            d: ["{\\tfrac{\\mathrm{d}#1}{\\mathrm{d}#2}}", 2],
            p: ["{\\tfrac{\\partial #1}{\\partial #2}}", 2],
    	}
    }
});
</script>

<style type="text/css">
  .gist {width: 100% !important;}
  .gist-file
  .gist-data {max-width: 100%;}
</style>

</head>
<body class="post-template">

    <div class="site-wrapper">

        

<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
                <a class="site-nav-logo" href="../">Deep Learning</a>
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="../">Home</a></li>
    <li class="nav-tag" role="menuitem"><a href="http://localhost:2368/tag/getting-started/">Tag</a></li>
    <li class="nav-author" role="menuitem"><a href="http://localhost:2368/author/ghost/">Author</a></li>
    <li class="nav-help" role="menuitem"><a href="https://docs.ghost.org">Help</a></li>
</ul>

    </div>
    <div class="site-nav-right">
        <div class="social-links">
                <a class="social-link social-link-fb" href="https://www.facebook.com/ghost" title="Facebook" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"></path></svg>
</a>
                <a class="social-link social-link-tw" href="https://twitter.com/tryghost" title="Twitter" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"></path></svg>
</a>
        </div>
            <a class="rss-button" href="https://feedly.com/i/subscription/feed/http://localhost:2368/rss/" title="RSS" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24"><circle cx="6.18" cy="17.82" r="2.18"></circle><path d="M4 4.44v2.83c7.03 0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56zm0 5.66v2.83c3.9 0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9z"></path></svg>
</a>
    </div>
</nav>
    </div>
</header>


<main id="site-main" class="site-main outer">
    <div class="inner">

        <article class="post-full post ">

            <header class="post-full-header">
                <section class="post-full-meta">
                    <time class="post-full-meta-date" datetime="2019-05-28">28 May 2019</time>
                </section>
                <h1 class="post-full-title">Linear Regression With Neurons (1)</h1>
            </header>

            <figure class="post-full-image">
                <img srcset="https://images.unsplash.com/photo-1511268594014-0e9d3ea5c33e?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1080&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ 300w,
                            https://images.unsplash.com/photo-1511268594014-0e9d3ea5c33e?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1080&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ 600w,
                            https://images.unsplash.com/photo-1511268594014-0e9d3ea5c33e?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1080&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ 1000w,
                            https://images.unsplash.com/photo-1511268594014-0e9d3ea5c33e?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1080&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ 2000w" sizes="(max-width: 800px) 400px,
                            (max-width: 1170px) 700px,
                            1400px" src="https://images.unsplash.com/photo-1511268594014-0e9d3ea5c33e?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1080&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" alt="Linear Regression With Neurons (1)">
            </figure>

            <section class="post-full-content">
                <div class="post-content">
                    <!--kg-card-begin: markdown--><h1 id="whatareneurons">What are neurons?</h1>
<br>
<p>The goal of this post is to introduce <strong>artificial neurons</strong> — the basic building blocks of a neural network — as well as the way in which they learn. Their general concept is based on their namesake, the neurons of the human brain.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: image--><figure class="kg-card kg-image-card kg-card-hascaption"><img src="../content/images/2019/05/biological-neuron-1.jpg" class="kg-image"><figcaption>Biological Neurons</figcaption></figure><!--kg-card-end: image--><!--kg-card-begin: markdown--><p>I'm no expert in cognitive science, but from what I understand a biological neuron receives a number of electrical impulses from surrounding neurons. Then, based on the cumulative intensity of these signals, the neuron might send out an electrical impulse of its own. This impulse is then transmitted to other neurons, which each decide whether to fire, ad infinitum.</p>
<p>The mechanism which determines whether a particular neuron will fire given a set of input signals is still mysterious, but luckily we are not too concerned with it. We care only about the general behavior: a neuron takes a set of inputs, processes them in some way, and returns an output. This is exactly what our artificial neurons will do, just in a more transparent way.</p>
<p>In the next post, I will give the full definition of an artificial neuron. For now, let's look at a minimal example — a neuron with a single input and a single parameter:</p>
<p><img src="../content/images/2019/05/one-parameter-neuron.svg" alt="one-parameter-neuron"></p>
<p>This neuron takes a single number $x$ as an input, multiplies it by a <strong>weight</strong> $w$, and returns their product, $wx$.</p>
<p>It is important to note that the neuron's weight $w$ is part of the neuron itself, and thus it remains fixed no matter what input $x$ is fed to the neuron. However, the input can be any number. So, for instance, if $w=5$, then the neuron outputs $5x$ for any input $x$.</p>
<p>So technically this neuron is just the function $N:\R\to\R$ defined by $N(x) = wx$, where $w\in\R$ is fixed. It is often useful to keep this function representation in the back of our minds, but we will prefer the graphical interpretation where a neuron is represented by a node, connected to inputs and outputs.</p>
<h1 id="howdoneuronslearn">How do neurons learn?</h1>
<p>Now that you know how our one-parameter, single input neuron acts on inputs, it's time to see how we can train our neuron to make meaningful predictions. I will demonstrate this with the simplest example I can think of.</p>
<p>Let's say we're given the following data, collected from three people at the grocery store, where $x$ represents the number of items they purchased and $y$ represents how much they paid, in dollars:</p>
<p>$$\begin{array}{|c|c|c|}<br>
\hline<br>
\text{Person} &amp; x = \text{Items} &amp; y = \text{Price} \\<br>
\hline<br>
\text{Sally}  &amp; 1                &amp; 2                \\<br>
\hline<br>
\text{Adam}   &amp; 2                &amp; 4                \\<br>
\hline<br>
\text{Fred}   &amp; 3                &amp; 6                \\<br>
\hline<br>
\end{array}$$</p>
<p>Seeing this table, it might be immediately apparent to you that $y=2x$ for each $x$ in the table. In practice, however, we are often faced with much larger amounts of data that do not follow such a clear trend. We would thus like our neuron to learn for itself how best to make predictions, based on the data. So in this case, we would like the neuron to figure out that it should set $w=2$.</p>
<p>As simple as the problem is, this is easier said than done. But let's walk through training the neuron on our data set, step by step. To start off, we need to pick some value for $w$, so the neuron can actually do anything at all. Let's choose $w=0$, and see what predictions our neuron makes:</p>
<p>$$\begin{array}{|c|c|c|}<br>
\hline<br>
x &amp; \hat{y} = wx &amp; y \\<br>
\hline<br>
1 &amp; 0            &amp; 2 \\<br>
\hline<br>
2 &amp; 0            &amp; 4 \\<br>
\hline<br>
3 &amp; 0            &amp; 6 \\<br>
\hline<br>
\end{array}$$</p>
<p>Here, $\hat{y}$ represents the prediction made given the input value $x$, and $y$ represents the true value from our data set. It's safe to say that choosing $w=0$ is not giving us very good predictions. We would like to adjust $w$ so that the neuron does a better job. In order to do this, we need some way to measure just how badly the neuron is currently performing.</p>
<p>We define the <strong>loss</strong> of the neuron for a single input value $x_i$ as follows:</p>
<p>$$L(x_i) = (y_i - \hat{y_i})^2.$$</p>
<p>This loss function $L$ basically tells us how badly the neuron is doing at predicting a single value $x_i$. Notice that if the prediction $\hat{y_i}$ is close to the actual value $y_i$, the loss will be small (positive, but close to zero). If the neuron is perfectly predicting the output, the loss will be exactly zero. If the neuron is given an input $x_i$ and spits out a terrible answer $\hat{y}_i$ that is very far from the true answer $y$, then the loss for that input will be huge.</p>
<p>In the case of our example,</p>
<p>$$\begin{align}<br>
L(x_1) &amp;= (y_1 - \hat{y}_1)^2 = (2 - 0)^2 = 4, \\<br>
L(x_2) &amp;= (y_2 - \hat{y}_2)^2 = (4 - 0)^2 = 16, \\<br>
L(x_3) &amp;= (y_3 - \hat{y}_3)^2 = (6 - 0)^2 = 36.<br>
\end{align}$$</p>
<p>Notice that the loss for $x_1$ is small because the prediction is not too far from the actual result. As the predictions get further off, the losses grow exponentially.</p>
<p>The loss function we've just defined is called the <strong>squared error</strong>, for obvious reasons. We could have defined a different loss function, if we'd wanted. For instance, we could have taken $L(x_i)=\abs{y_i - \hat{y}_i}$. This is called <strong>absolute error</strong>, but it is not used very often for reasons we will see shortly. There are plenty of other choices as well, but squared error will do just fine for now.</p>
<p>Now that we have a measure for how poorly the neuron is performing for a single input, how can we measure how poorly the neuron is doing overall? The answer is simple — we can just average all the losses.</p>
<p>We define the <strong>cost</strong> of the neuron to be</p>
<p>$$C = \frac{1}{m}\sum_{i=1}^m L(x_i),$$</p>
<p>where $m$ is the total number of data points (so in our case, $m=3$). This is literally just the average loss, computed over all the data. In our case,</p>
<p>$$\begin{align}<br>
C &amp;= \tfrac{1}{3}\big(L(x_1)+L(x_2)+L(x_2)\big) \\<br>
&amp;= \tfrac{1}{3}(4 + 16 + 36) \\<br>
&amp;= \tfrac{56}{3} \\<br>
&amp;\approx 18.6666666667.<br>
\end{align}$$</p>
<p>Because we have chosen our loss function to be the squared error, and our cost is the mean of the losses, this cost function is called <strong>mean squared error</strong>.</p>
<p>So the goal now becomes <em>minimizing the cost</em> of our neuron! Once we have settled on a weight which makes the cost sufficiently low, we say our neuron is <strong>trained</strong>.</p>
<p>Okay, so our neuron currently has a cost of $18.6666666667$. This doesn't really tell us too much by itself. We want the network to choose a new weight which will result in a lower cost. How can we accomplish this?</p>
<p>One idea would be to keep randomly selecting weights and keeping track of which results in the lowest cost, i.e., is giving the best predictions according to our metric.</p>
<blockquote>
<p><strong>Algorithm 1. Random Guessing</strong></p>
<ol>
<li>Decide on some reasonable interval $[a, b]$ to search.</li>
<li>Initialize <code>best_w = 0</code> and <code>best_cost = infinity</code> (or some very large number).</li>
<li>Pick <code>w</code> randomly in the interval $[a, b]$.</li>
<li>Compute the cost <code>C</code> of the neuron with the weight <code>w</code>.</li>
<li>If <code>C</code> is smaller than <code>best_cost</code>, set <code>best_cost = C</code> and <code>best_w = w</code>.</li>
<li>Go back to step 3 and repeat for as long as you like.</li>
</ol>
</blockquote>
<p>But this is not really a good solution, since there are infinite numbers to choose from and the neuron has no idea where to look. (And in fact, in the general case of a network with many neurons it will probably never find decent weights.)</p>
<p>What the neuron really needs is some context. Maybe it could try out two new weights, one slightly less than $0$ and one slightly greater than $0$, and see which weight results in a lower cost. So for instance, maybe we could try $-0.1$ and $0.1$. I won't bore you with the details of the computation, but what we end up with is:</p>
<p>$$\begin{align}<br>
C_{-0.1} &amp;= 20.58, \\[.5em]<br>
C_{0.1}  &amp;= 16.8466666667.<br>
\end{align}$$</p>
<p>What this tells us is that we can get better performance out of our neuron if we increase the weight $w$ from $0$ to $0.1$, whereas if we had decreased the weight then the neuron would have given us worse predictions. This seems correct, since we know in the back of our minds that the value of $w$ that would give perfect predictions is actually $2$, so we certainly want it to increase from $0$. We could keep repeating this process until shifting the weight in either direction would give a worse (higher) cost, and then we would be done.</p>
<blockquote>
<p><strong>Algorithm 2. Weight Sliding</strong></p>
<ol>
<li>Initialize <code>w = 0</code> and pick a small increment <code>increment</code>.</li>
<li>Compute the costs <code>C_minus</code>, <code>C</code> and <code>C_plus</code> for the weights <code>w</code>, <code>w - increment</code> and <code>w + increment</code>, respectively.</li>
<li>If <code>C_minus</code> is less than <code>C</code> and <code>C_plus</code>, update <code>C = C_minus</code> and <code>w = w - increment</code>.</li>
<li>If instead <code>C_plus</code> is less than <code>C</code> and <code>C_minus</code>, update <code>C = C_plus</code> and <code>w = w + increment</code>.</li>
<li>Otherwise, <code>C</code> is less than <code>C_minus</code> and <code>C_plus</code>, so we are done.</li>
<li>Go back to step 2, but don't bother recomputing <code>C</code>.</li>
</ol>
</blockquote>
<p>This isn't such a bad method. It will usually tell us which direction we should nudge the weight to improve the neuron's predictions. However, it always takes fixed-size steps, it is not foolproof and it does poorly from a computational standpoint — it requires us to compute the cost three times for each choice of weight, and compare them. It turns out there's a much better way!</p>
<h1 id="gradientdescent">Gradient descent</h1>
<p>It turns out that we can figure out which direction to shift the weight, and by how far, with a single computation. The actual method requires some knowledge of calculus, but the basic ideas should make sense to anyone following along. Just don't get scared off by the derivatives if you don't know what they are.</p>
<p>Here is a plot of the cost for our neuron for a whole interval of possible weights. I computed this by calculating the cost associated with a bunch of weights in the interval $[-1, 5]$.</p>
<iframe width="100%" height="400" frameborder="0" scrolling="no" src="https://plot.ly/~eshapiro/18.embed?showlink=false"></iframe>
<p>Computing this whole graph is similar to Algorithm 2 above. That is, it's very inefficient and computationally intensive. The graph is solely for the sake of our visualizing the problem. Our neuron does not know the shape of the cost function, so it needs some other way update its weight for a better prediction.</p>
<p>The inspiration behind the gradient descent algorithm comes from the idea of a ball rolling down a hill. The idea is that gravity will always pull the ball in the direction of <em>steepest descent</em>. For a visualization, press "Play" on the animation below.</p>
<iframe width="100%" height="400" frameborder="0" scrolling="no" src="https://plot.ly/~eshapiro/82.embed?showlink=false"></iframe>
<p>Of course, in real life the momentum of the ball would keep it moving to the right for a little while after it reached the bottom, then it would change directions and oscillate for a short time before finally settling at the bottom. But we don't care too much about the details, only the general idea.</p>
<p>It just so happens that we have a way to compute, at any given point, the direction of steepest descent. Recall that the derivative of a function at a point (if it exists) tells us geometrically the slope of the tangent line to the function's curve at that point. Here's what the tangent line to the cost curve looks like when $w=0$:</p>
<iframe width="100%" height="400" frameborder="0" scrolling="no" src="https://plot.ly/~eshapiro/83.embed?showlink=false"></iframe>
<p>The slope of this tangent line is negative, implying that the cost is decreasing as we move to the right. This means that if the derivative is negative, we want to shift our weight to the right. And if the derivative is positive, we want the weight to shift to the left.</p>
<p>Since the derivative at $w=0$ is negative, we shift $w$ to the right a bit, then reevaluate:</p>
<iframe width="100%" height="400" frameborder="0" scrolling="no" src="https://plot.ly/~eshapiro/85.embed?showlink=false"></iframe>
<p>The derivative is still negative, but it's <em>less</em> negative. So we should continue to shift $w$ to the right, but maybe not as much this time.</p>
<p>If we continue this process, we should eventually shift $w$ to $2$, where the derivative is $0$. That is, the cost has a horizontal tangent line at this point. This is what tells us we have found a weight that minimzes the cost!</p>
<p>The amount that we shift $w$ by in each iteration should be <em>proportional</em> to the magnitude of the derivative. That way it will move by a lot more when the cost is very high and a lot less when the cost is very low, so we don't overshoot our target by too much. Said another way, this will guarantee that if our weight is close to optimal, we shift it by a smaller amount than if it is far from optimal. We call this proportionality constant the learning rate, and I will talk more about learning rates in another post.</p>
<p>So here's the algorithm:</p>
<blockquote>
<p><strong>Algorithm 3. Gradient Descent</strong></p>
<ol>
<li>Pick an initial weight <code>w</code> using some predetermined method, and a small learning rate <code>learning_rate</code>.</li>
<li>Compute the derivative of the cost with respect to <code>w</code>, call it <code>D_w</code>.</li>
<li>Update <code>w = w - learning_rate * D_w</code>.</li>
<li>Go back to step 2 and repeat for as long as you like, or until <code>D_w</code> is zero.</li>
</ol>
</blockquote>
<p>This algorithm, as simple as it may seem, forms the basis for how neural networks learn from data.</p>
<h1 id="computingthegradient">Computing the gradient</h1>
<p>Since our neuron only has one weight to optimize, in this case 'gradient' just means derivative. We will shortly see an example where this is no longer the case. But for now, we need to figure out a way of calculating the derivative of our cost function for a specific value of $w$. That is, we need to find $\d{C}{w}$.</p>
<p>Recall that $C$ is a function of the losses $L_i$, which are in turn functions of the outputs $\hat{y_i}$, which are in turn functions of the inputs $x_i$. Here are the equations governing these relationships:</p>
<p>$$\begin{align}<br>
C &amp;= \frac{1}{m}\sum_{i=1}^m L_i, \\<br>
L_i &amp;= (y_i - \hat{y_i})^2, \\[1em]<br>
\hat{y}_i &amp;= wx_i.<br>
\end{align}$$</p>
<p>The chain rule tells us precisely how to calculate $\d{C}{w}$ from these equations! Through repeated use of the chain rule, we see that:</p>
<p>$$\begin{align}<br>
\d{C}{w} &amp;= \sum_{i=1}^m\p{C}{L_i}\d{L_i}{w} \\<br>
&amp;= \sum_{i=1}^m\p{C}{L_i}\d{L_i}{\hat{y_i}}\d{\hat{y_i}}{w}.<br>
\end{align}$$</p>
<p>These three quantities are easy to calculate from the equations above:</p>
<p>$$\begin{align}<br>
\p{C}{L_i} &amp;= \tfrac{1}{m}, \\<br>
\d{L_i}{\hat{y_i}} &amp;= -2(y_i-\hat{y_i}), \\<br>
\d{\hat{y_i}}{w} &amp;= x_i.<br>
\end{align}$$</p>
<p>Combining these using the chain rule, we see that</p>
<p>$$\d{C}{w} = -\frac{2}{m} \sum_{i=1}^m (y_i-\hat{y_i})x_i.$$</p>
<p>This means we can use the following algorithm to compute the gradient of our cost function for a particular value of $w$:</p>
<blockquote>
<p><strong>Algorithm 4. Gradient Computation</strong></p>
<ol>
<li>Initialize <code>D_w = 0</code>, set <code>x</code> to the first input and <code>y</code> to its corresponding output in our data set, and set <code>m</code> to the number of data points.</li>
<li>Compute the neuron's output <code>y_hat = w * x</code>.</li>
<li>Compute the contribution to <code>D_w</code> from this data point, <code>D_w = D_w - 2 * (y - y_hat) * x</code>.</li>
<li>Set <code>x</code> to the next input and <code>y</code> to its corresponding output, go back to step 2 and repeat until we have exhausted all our data.</li>
<li>Set <code>D_w = D_w / m</code>.</li>
</ol>
</blockquote>
<p>For this one-parameter network, it is easy to explicitly compute the optimal weight, since we have a formula for $\d{C}{w}$. From calculus, we know that the cost's minimum must occur when $\d{C}{w}=0$. That is, when</p>
<p>$$\begin{align}<br>
0 &amp;= -\frac{2}{m} \sum_{i=1}^m (y_i-\hat{y_i})x_i \\<br>
&amp;= -\frac{2}{m} \sum_{i=1}^m (y_i-wx_i)x_i \\<br>
&amp;= -\frac{2}{m} \sum_{i=1}^m y_ix_i + \frac{2w}{m} \sum_{i=1}^m x_i^2.<br>
\end{align}$$</p>
<p>This is easily solved for $w$:</p>
<p>$$w = \frac{\sum_{i=1}^m y_ix_i}{\sum_{i=1}^m x_i^2}.$$</p>
<p>This technique for finding $w$ is conventionally called <strong>least squares linear regression through the origin</strong>. In the case of our example, this becomes, as expected,</p>
<p>$$\begin{align}<br>
w &amp;= \tfrac{2\cdot 1 + 4\cdot 2 + 6\cdot 3}{1^2 + 2^2 + 3^2} \\<br>
&amp;= \tfrac{28}{14} \\[.5em]<br>
&amp;= 2.<br>
\end{align}$$</p>
<p>So our one-parameter neuron is really no match for what we can do analytically. But that's not the point. When we work with more complicated neurons, and link them together into neural networks, they quickly begin to solve problems that we do not have other mathematical tools to solve. And optimizing their parameters becomes impossible to do analytically, but gradient descent has no problem with it!</p>
<h1 id="puttingittogether">Putting it together</h1>
<p>We have now completely described our one-parameter neuron and how it learns! Below is some Python code which encapsulates our model and allows us to train a neuron given input and output data. The first class, <code>Neuron</code>, is a base class which we will continue to use as we implement more complex neurons. The second, <code>OneParameterNeuron</code>, inherits from <code>Neuron</code> and is a complete model of our one-parameter neuron.</p>
<script src="https://gist.github.com/eshapiro42/639a1d69d8239a23318ca2bb6cf8b934.js"></script>
<br>
<p>Calling the <code>train</code> method for a neuron launches 100 iterations, or <strong>epochs</strong>, of gradient descent. It defaults to zero as an initial choice of weight, and uses a learning rate of $0.01$.</p>
<p>Let's see it in action!</p>
<iframe height="600px" width="100%" src="https://repl.it/@EricShapiro/Linear-Regression-with-Neurons-1?lite=true" scrolling="no" frameborder="no" allowtransparency="true" allowfullscreen="true" sandbox="allow-forms allow-pointer-lock allow-popups allow-same-origin allow-scripts allow-modals"></iframe>
<br>
<p>It actually works! Look how small our model got the cost after just 100 epochs of gradient descent! And look how close it got to the optimal value of $w=2$.</p>
<p>There are some huge improvements that could be made to this code, in particular the <code>cost</code> and <code>compute_gradient</code> methods could be drastically sped up. However, this accurately reflects the algorithms we've discussed so far.</p>
<p>It could be interesting to try training our neuron on a larger, not-so-perfect data set. We'll use this data, which consists of 1000 points which follow the same general trend as our previous data, but no longer sit perfectly on the line $y=2x$. This is more like data we might encounter in the real world!</p>
<iframe width="100%" height="400" frameborder="0" scrolling="no" src="https://plot.ly/~eshapiro/89.embed?showlink=false"></iframe>
<p>Watch what happens when we train our neuron with this new data:</p>
<iframe height="600px" width="100%" src="https://repl.it/@EricShapiro/Linear-Regression-with-Neurons-2?lite=true" scrolling="no" frameborder="no" allowtransparency="true" allowfullscreen="true" sandbox="allow-forms allow-pointer-lock allow-popups allow-same-origin allow-scripts allow-modals"></iframe>
<br>
<p>It still did a pretty good job! Of course, it can't get the cost down as low anymore, but that's because the data is no longer perfectly linear and so there is some unavoidable error in any linear model of the data. Here is the trendline that our model predicted, superimposed on the data:</p>
<iframe width="100%" height="400" frameborder="0" scrolling="no" src="https://plot.ly/~eshapiro/93.embed?showlink=false"></iframe>
<br>
<p>I'd say that looks pretty good! As a comparison, least squares linear regression through the origin predicts a slope of $1.95078950636752$ for this same data.</p>
<p>This post is getting pretty long, so I'll leave it at that for now. In my next post, we'll improve our neuron by adding a second parameter, and we'll see how this makes everything a bit more complicated.</p>
<!--kg-card-end: markdown-->
                </div>
            </section>


            <footer class="post-full-footer">


                    
<section class="author-card">
        <span class="avatar-wrapper"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="M3.513 18.998C4.749 15.504 8.082 13 12 13s7.251 2.504 8.487 5.998C18.47 21.442 15.417 23 12 23s-6.47-1.558-8.487-4.002zM12 12c2.21 0 4-2.79 4-5s-1.79-4-4-4-4 1.79-4 4 1.79 5 4 5z" fill="#FFF"></path></g></svg>
</span>
    <section class="author-card-content">
        <h4 class="author-card-name"><a href="../author/eric/">Eric Shapiro</a></h4>
            <p>Read <a href="../author/eric/">more posts</a> by this author.</p>
    </section>
</section>
<div class="post-full-footer-right">
    <a class="author-card-button" href="../author/eric/">Read More</a>
</div>


            </footer>


        </article>

    </div>
</main>

<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">

                <article class="post-card post ">

    <a class="post-card-image-link" href="../linear-regression-with-neurons-2/">
        <img class="post-card-image" srcset="https://images.unsplash.com/photo-1499626662328-b83a935441a4?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1080&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ 300w,
                    https://images.unsplash.com/photo-1499626662328-b83a935441a4?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1080&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ 600w,
                    https://images.unsplash.com/photo-1499626662328-b83a935441a4?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1080&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ 1000w,
                    https://images.unsplash.com/photo-1499626662328-b83a935441a4?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1080&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ 2000w" sizes="(max-width: 1000px) 400px, 700px" src="https://images.unsplash.com/photo-1499626662328-b83a935441a4?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1080&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" alt="Linear Regression with Neurons (2)">
    </a>

    <div class="post-card-content">

        <a class="post-card-content-link" href="../linear-regression-with-neurons-2/">

            <header class="post-card-header">
                <h2 class="post-card-title">Linear Regression with Neurons (2)</h2>
            </header>

            <section class="post-card-excerpt">
                <p>More complicated data Last time, I introduced a simple artificial neuron with a single parameter, $w$. We figured out how, given labeled training data[1] that was reasonably linear and passed through the</p>
            </section>

        </a>

        <footer class="post-card-meta">

            <ul class="author-list">
                <li class="author-list-item">

                    <div class="author-name-tooltip">
                        Eric Shapiro
                    </div>

                        <a href="../author/eric/" class="static-avatar author-profile-image"><svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="M3.513 18.998C4.749 15.504 8.082 13 12 13s7.251 2.504 8.487 5.998C18.47 21.442 15.417 23 12 23s-6.47-1.558-8.487-4.002zM12 12c2.21 0 4-2.79 4-5s-1.79-4-4-4-4 1.79-4 4 1.79 5 4 5z" fill="#FFF"></path></g></svg>
</a>
                </li>
            </ul>

            <span class="reading-time">7 min read</span>

        </footer>

    </div>

</article>


        </div>
    </div>
</aside>

<div class="floating-header">
    <div class="floating-header-logo">
        <a href="../">
            <span>Deep Learning</span>
        </a>
    </div>
    <span class="floating-header-divider">—</span>
    <div class="floating-header-title">Linear Regression With Neurons (1)</div>
    <div class="floating-header-share">
        <div class="floating-header-share-label">Share this <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24">
    <path d="M7.5 15.5V4a1.5 1.5 0 1 1 3 0v4.5h2a1 1 0 0 1 1 1h2a1 1 0 0 1 1 1H18a1.5 1.5 0 0 1 1.5 1.5v3.099c0 .929-.13 1.854-.385 2.748L17.5 23.5h-9c-1.5-2-5.417-8.673-5.417-8.673a1.2 1.2 0 0 1 1.76-1.605L7.5 15.5zm6-6v2m-3-3.5v3.5m6-1v2"></path>
</svg>
</div>
        <a class="floating-header-share-tw" href="https://twitter.com/share?text=Linear%20Regression%20With%20Neurons%20(1)&amp;url=http://localhost:2368/linear-regression-with-neurons/" onclick="window.open(this.href, 'share-twitter', 'width=550,height=235');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"></path></svg>
        </a>
        <a class="floating-header-share-fb" href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:2368/linear-regression-with-neurons/" onclick="window.open(this.href, 'share-facebook','width=580,height=296');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"></path></svg>
        </a>
    </div>
    <progress id="reading-progress" class="progress" value="0">
        <div class="progress-container">
            <span class="progress-bar"></span>
        </div>
    </progress>
</div>




        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="../">Deep Learning</a> © 2019</section>
                <nav class="site-footer-nav">
                    <a href="../">Latest Posts</a>
                    <a href="https://www.facebook.com/ghost" target="_blank" rel="noopener">Facebook</a>
                    <a href="https://twitter.com/tryghost" target="_blank" rel="noopener">Twitter</a>
                    <a href="https://ghost.org" target="_blank" rel="noopener">Ghost</a>
                </nav>
            </div>
        </footer>

    </div>


    <script>
        var images = document.querySelectorAll('.kg-gallery-image img');
        images.forEach(function (image) {
            var container = image.closest('.kg-gallery-image');
            var width = image.attributes.width.value;
            var height = image.attributes.height.value;
            var ratio = width / height;
            container.style.flex = ratio + ' 1 0%';
        })
    </script>


    <script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="../assets/built/jquery.fitvids.js?v=8d5645c3cd"></script>


    <script>

// NOTE: Scroll performance is poor in Safari
// - this appears to be due to the events firing much more slowly in Safari.
//   Dropping the scroll event and using only a raf loop results in smoother
//   scrolling but continuous processing even when not scrolling
$(document).ready(function () {
    // Start fitVids
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
    // End fitVids

    var progressBar = document.querySelector('#reading-progress');
    var header = document.querySelector('.floating-header');
    var title = document.querySelector('.post-full-title');

    var lastScrollY = window.scrollY;
    var lastWindowHeight = window.innerHeight;
    var lastDocumentHeight = $(document).height();
    var ticking = false;

    function onScroll() {
        lastScrollY = window.scrollY;
        requestTick();
    }

    function onResize() {
        lastWindowHeight = window.innerHeight;
        lastDocumentHeight = $(document).height();
        requestTick();
    }

    function requestTick() {
        if (!ticking) {
            requestAnimationFrame(update);
        }
        ticking = true;
    }

    function update() {
        var trigger = title.getBoundingClientRect().top + window.scrollY;
        var triggerOffset = title.offsetHeight + 35;
        var progressMax = lastDocumentHeight - lastWindowHeight;

        // show/hide floating header
        if (lastScrollY >= trigger + triggerOffset) {
            header.classList.add('floating-active');
        } else {
            header.classList.remove('floating-active');
        }

        progressBar.setAttribute('max', progressMax);
        progressBar.setAttribute('value', lastScrollY);

        ticking = false;
    }

    window.addEventListener('scroll', onScroll, {passive: true});
    window.addEventListener('resize', onResize, false);

    update();

});
</script>


    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.14.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.14.0/plugins/line-numbers/prism-line-numbers.min.js"></script>
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.14.0/themes/prism.min.css">
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.14.0/plugins/line-numbers/prism-line-numbers.min.css">
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.14.0/themes/prism-coy.min.css">

<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.14.0/components/prism-csharp.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.14.0/components/prism-css.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.14.0/components/prism-c.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.14.0/components/prism-clike.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.14.0/components/prism-python.min.js"></script>

</body>
